{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNF6ClZlHTiEkiUpdOvxiVI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NidaNabi/cnn_from_scratch/blob/main/cnnpytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class ConvolutionLayer:\n",
        "    def __init__(self, kernel_num, kernel_size):\n",
        "        self.kernel_num = kernel_num\n",
        "        self.kernel_size = kernel_size\n",
        "       \n",
        "        self.kernels = np.random.randn(kernel_num, kernel_size, kernel_size) / (kernel_size**2)\n",
        "\n",
        "    def patches_generator(self, image):\n",
        "        image_h, image_w = image.shape\n",
        "        self.image = image\n",
        "        for h in range(image_h-self.kernel_size+1):\n",
        "            for w in range(image_w-self.kernel_size+1):\n",
        "                patch = image[h:(h+self.kernel_size), w:(w+self.kernel_size)]\n",
        "                yield patch, h, w\n",
        "    \n",
        "    def forward_prop(self, image):\n",
        "        image_h, image_w = image.shape\n",
        "        convolution_output = np.zeros((image_h-self.kernel_size+1, image_w-self.kernel_size+1, self.kernel_num))\n",
        "        for patch, h, w in self.patches_generator(image):\n",
        "            convolution_output[h,w] = np.sum(patch*self.kernels, axis=(1,2))\n",
        "        return convolution_output\n",
        "    \n",
        "    def back_prop(self, dE_dY, alpha):\n",
        "        dE_dk = np.zeros(self.kernels.shape)\n",
        "        for patch, h, w in self.patches_generator(self.image):\n",
        "            for f in range(self.kernel_num):\n",
        "                dE_dk[f] += patch * dE_dY[h, w, f]\n",
        "        self.kernels -= alpha*dE_dk\n",
        "        return dE_dk\n"
      ],
      "metadata": {
        "id": "OPsmYHXcC4-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPoolingLayer:\n",
        "    def __init__(self, kernel_size):\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def patches_generator(self, image):\n",
        "        output_h = image.shape[0] // self.kernel_size\n",
        "        output_w = image.shape[1] // self.kernel_size\n",
        "        self.image = image\n",
        "\n",
        "        for h in range(output_h):\n",
        "            for w in range(output_w):\n",
        "                patch = image[(h*self.kernel_size):(h*self.kernel_size+self.kernel_size), (w*self.kernel_size):(w*self.kernel_size+self.kernel_size)]\n",
        "                yield patch, h, w\n",
        "\n",
        "    def forward_prop(self, image):\n",
        "        image_h, image_w, num_kernels = image.shape\n",
        "        max_pooling_output = np.zeros((image_h//self.kernel_size, image_w//self.kernel_size, num_kernels))\n",
        "        for patch, h, w in self.patches_generator(image):\n",
        "            max_pooling_output[h,w] = np.amax(patch, axis=(0,1))\n",
        "        return max_pooling_output\n",
        "\n",
        "    def back_prop(self, dE_dY):\n",
        "        dE_dk = np.zeros(self.image.shape)\n",
        "        for patch,h,w in self.patches_generator(self.image):\n",
        "            image_h, image_w, num_kernels = patch.shape\n",
        "            max_val = np.amax(patch, axis=(0,1))\n",
        "\n",
        "            for idx_h in range(image_h):\n",
        "                for idx_w in range(image_w):\n",
        "                    for idx_k in range(num_kernels):\n",
        "                        if patch[idx_h,idx_w,idx_k] == max_val[idx_k]:\n",
        "                            dE_dk[h*self.kernel_size+idx_h, w*self.kernel_size+idx_w, idx_k] = dE_dY[h,w,idx_k]\n",
        "            return dE_dk\n"
      ],
      "metadata": {
        "id": "30vpu8EDD4MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxLayer:\n",
        "    def __init__(self, input_units, output_units):\n",
        "        self.weight = np.random.randn(input_units, output_units)/input_units\n",
        "        self.bias = np.zeros(output_units)\n",
        "\n",
        "    def forward_prop(self, image):\n",
        "        self.original_shape = image.shape \n",
        "        image_flattened = image.flatten()\n",
        "        self.flattened_input = image_flattened \n",
        "        first_output = np.dot(image_flattened, self.weight) + self.bias\n",
        "        self.output = first_output\n",
        "        softmax_output = np.exp(first_output) / np.sum(np.exp(first_output), axis=0)\n",
        "        return softmax_output\n",
        "\n",
        "    def back_prop(self, dE_dY, alpha):\n",
        "        for i, gradient in enumerate(dE_dY):\n",
        "            if gradient == 0:\n",
        "                continue\n",
        "            transformation_eq = np.exp(self.output)\n",
        "            S_total = np.sum(transformation_eq)\n",
        "\n",
        "           \n",
        "            dY_dZ = -transformation_eq[i]*transformation_eq / (S_total**2)\n",
        "            dY_dZ[i] = transformation_eq[i]*(S_total - transformation_eq[i]) / (S_total**2)\n",
        "\n",
        "       \n",
        "            dZ_dw = self.flattened_input\n",
        "            dZ_db = 1\n",
        "            dZ_dX = self.weight\n",
        "\n",
        "          \n",
        "            dE_dZ = gradient * dY_dZ\n",
        "\n",
        "          \n",
        "            dE_dw = dZ_dw[np.newaxis].T @ dE_dZ[np.newaxis]\n",
        "            dE_db = dE_dZ * dZ_db\n",
        "            dE_dX = dZ_dX @ dE_dZ\n",
        "\n",
        "           \n",
        "            self.weight -= alpha*dE_dw\n",
        "            self.bias -= alpha*dE_db\n",
        "\n",
        "            return dE_dX.reshape(self.original_shape)\n",
        "\n",
        "def CNN_forward(image, label, layers):\n",
        "    output = image/255.\n",
        "    for layer in layers:\n",
        "        output = layer.forward_prop(output)\n",
        "  \n",
        "    loss = -np.log(output[label])\n",
        "    accuracy = 1 if np.argmax(output) == label else 0\n",
        "    return output, loss, accuracy\n",
        "\n",
        "def CNN_backprop(gradient, layers, alpha=0.05):\n",
        "    grad_back = gradient\n",
        "    for layer in layers[::-1]:\n",
        "        if type(layer) in [ConvolutionLayer, SoftmaxLayer]:\n",
        "            grad_back = layer.back_prop(grad_back, alpha)\n",
        "        elif type(layer) == MaxPoolingLayer:\n",
        "            grad_back = layer.back_prop(grad_back)\n",
        "    return grad_back\n",
        "\n",
        "\n",
        "def CNN_training(image, label, layers, alpha=0.05):\n",
        "   \n",
        "    output, loss, accuracy = CNN_forward(image, label, layers)\n",
        "\n",
        "  \n",
        "    gradient = np.zeros(10)\n",
        "    gradient[label] = -1/output[label]\n",
        "\n",
        "    gradient_back = CNN_backprop(gradient, layers, alpha)\n",
        "\n",
        "    return loss, accuracy"
      ],
      "metadata": {
        "id": "knw1N-hoEEHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def main():\n",
        "\n",
        "  (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "  X_train = X_train[:5000]\n",
        "  y_train = y_train[:5000]\n",
        "\n",
        "  layers = [\n",
        "    ConvolutionLayer(16,3), \n",
        "    MaxPoolingLayer(2),\n",
        "    SoftmaxLayer(13*13*16, 10) \n",
        "    ] \n",
        "\n",
        "  for epoch in range(4):\n",
        "    print('Epoch {} ->'.format(epoch+1))\n",
        "  \n",
        "    permutation = np.random.permutation(len(X_train))\n",
        "    X_train = X_train[permutation]\n",
        "    y_train = y_train[permutation]\n",
        "  \n",
        "    loss = 0\n",
        "    accuracy = 0\n",
        "    for i, (image, label) in enumerate(zip(X_train, y_train)):\n",
        "      if i % 100 == 0:\n",
        "        print(\"Step {}. For the last 100 steps: average loss {}, accuracy {}\".format(i+1, loss/100, accuracy))\n",
        "        loss = 0\n",
        "        accuracy = 0\n",
        "      loss_1, accuracy_1 = CNN_training(image, label, layers)\n",
        "      loss += loss_1\n",
        "      accuracy += accuracy_1\n",
        "  \n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yu8TC_oDx4d",
        "outputId": "f0f4a1df-5689-496e-d71f-3140d0b08d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 ->\n",
            "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
            "Step 101. For the last 100 steps: average loss 1.7011401497323502, accuracy 47\n",
            "Step 201. For the last 100 steps: average loss 1.1349088318888045, accuracy 60\n",
            "Step 301. For the last 100 steps: average loss 1.0196926607571093, accuracy 75\n",
            "Step 401. For the last 100 steps: average loss 0.7782645278762215, accuracy 78\n",
            "Step 501. For the last 100 steps: average loss 0.7757236112260765, accuracy 72\n",
            "Step 601. For the last 100 steps: average loss 0.6462324631261612, accuracy 82\n",
            "Step 701. For the last 100 steps: average loss 0.46818440188619476, accuracy 87\n",
            "Step 801. For the last 100 steps: average loss 0.5982121453800602, accuracy 85\n",
            "Step 901. For the last 100 steps: average loss 0.6025854819841012, accuracy 82\n",
            "Step 1001. For the last 100 steps: average loss 0.5804550149477031, accuracy 79\n",
            "Step 1101. For the last 100 steps: average loss 0.3232015356616456, accuracy 88\n",
            "Step 1201. For the last 100 steps: average loss 0.4216723641397899, accuracy 87\n",
            "Step 1301. For the last 100 steps: average loss 0.34524568930558663, accuracy 88\n",
            "Step 1401. For the last 100 steps: average loss 0.4761575919123556, accuracy 89\n",
            "Step 1501. For the last 100 steps: average loss 0.3129095809319885, accuracy 88\n",
            "Step 1601. For the last 100 steps: average loss 0.4751196907464251, accuracy 87\n",
            "Step 1701. For the last 100 steps: average loss 0.5200581366943289, accuracy 83\n",
            "Step 1801. For the last 100 steps: average loss 0.39939389909521816, accuracy 88\n",
            "Step 1901. For the last 100 steps: average loss 0.32425979563606944, accuracy 91\n",
            "Step 2001. For the last 100 steps: average loss 0.40991636223108807, accuracy 88\n",
            "Step 2101. For the last 100 steps: average loss 0.44310843156914403, accuracy 89\n",
            "Step 2201. For the last 100 steps: average loss 0.447564760617622, accuracy 90\n",
            "Step 2301. For the last 100 steps: average loss 0.266616872507532, accuracy 93\n",
            "Step 2401. For the last 100 steps: average loss 0.32446144702812757, accuracy 90\n",
            "Step 2501. For the last 100 steps: average loss 0.5224612952250274, accuracy 83\n",
            "Step 2601. For the last 100 steps: average loss 0.20829423121925042, accuracy 96\n",
            "Step 2701. For the last 100 steps: average loss 0.42078545562858827, accuracy 85\n",
            "Step 2801. For the last 100 steps: average loss 0.3357540642072277, accuracy 87\n",
            "Step 2901. For the last 100 steps: average loss 0.37707868814077977, accuracy 87\n",
            "Step 3001. For the last 100 steps: average loss 0.410169924524402, accuracy 88\n",
            "Step 3101. For the last 100 steps: average loss 0.4499451378720955, accuracy 89\n",
            "Step 3201. For the last 100 steps: average loss 0.4755260825802016, accuracy 84\n",
            "Step 3301. For the last 100 steps: average loss 0.22071300382737796, accuracy 95\n",
            "Step 3401. For the last 100 steps: average loss 0.329468843370935, accuracy 91\n",
            "Step 3501. For the last 100 steps: average loss 0.33242466885648575, accuracy 93\n",
            "Step 3601. For the last 100 steps: average loss 0.4164976571580887, accuracy 86\n",
            "Step 3701. For the last 100 steps: average loss 0.4100247286438137, accuracy 85\n",
            "Step 3801. For the last 100 steps: average loss 0.23482701231013944, accuracy 95\n",
            "Step 3901. For the last 100 steps: average loss 0.31749266992799074, accuracy 87\n",
            "Step 4001. For the last 100 steps: average loss 0.3135836989276948, accuracy 93\n",
            "Step 4101. For the last 100 steps: average loss 0.28043786565552564, accuracy 90\n",
            "Step 4201. For the last 100 steps: average loss 0.38104228049681155, accuracy 90\n",
            "Step 4301. For the last 100 steps: average loss 0.3516629019770111, accuracy 86\n",
            "Step 4401. For the last 100 steps: average loss 0.3236322011979451, accuracy 91\n",
            "Step 4501. For the last 100 steps: average loss 0.34062513261103816, accuracy 92\n",
            "Step 4601. For the last 100 steps: average loss 0.3718109876302539, accuracy 89\n",
            "Step 4701. For the last 100 steps: average loss 0.34862221757393563, accuracy 89\n",
            "Step 4801. For the last 100 steps: average loss 0.48389926187127746, accuracy 83\n",
            "Step 4901. For the last 100 steps: average loss 0.25309080757122204, accuracy 92\n",
            "Epoch 2 ->\n",
            "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
            "Step 101. For the last 100 steps: average loss 0.269177806176654, accuracy 90\n",
            "Step 201. For the last 100 steps: average loss 0.18574009848147086, accuracy 93\n",
            "Step 301. For the last 100 steps: average loss 0.3116248857443382, accuracy 93\n",
            "Step 401. For the last 100 steps: average loss 0.4159211934744114, accuracy 88\n",
            "Step 501. For the last 100 steps: average loss 0.3803054378061937, accuracy 93\n",
            "Step 601. For the last 100 steps: average loss 0.3174161291436672, accuracy 94\n",
            "Step 701. For the last 100 steps: average loss 0.5043052077963904, accuracy 86\n",
            "Step 801. For the last 100 steps: average loss 0.13238945292086868, accuracy 97\n",
            "Step 901. For the last 100 steps: average loss 0.29317722214229625, accuracy 92\n",
            "Step 1001. For the last 100 steps: average loss 0.3719016096599694, accuracy 90\n",
            "Step 1101. For the last 100 steps: average loss 0.3288386458125877, accuracy 89\n",
            "Step 1201. For the last 100 steps: average loss 0.2754267984746875, accuracy 92\n",
            "Step 1301. For the last 100 steps: average loss 0.22056892773039372, accuracy 92\n",
            "Step 1401. For the last 100 steps: average loss 0.29981444105171057, accuracy 93\n",
            "Step 1501. For the last 100 steps: average loss 0.19946891277578097, accuracy 93\n",
            "Step 1601. For the last 100 steps: average loss 0.2876661574477627, accuracy 92\n",
            "Step 1701. For the last 100 steps: average loss 0.3300972218524215, accuracy 89\n",
            "Step 1801. For the last 100 steps: average loss 0.25329854485278813, accuracy 93\n",
            "Step 1901. For the last 100 steps: average loss 0.20250864644638322, accuracy 94\n",
            "Step 2001. For the last 100 steps: average loss 0.12395011304541426, accuracy 96\n",
            "Step 2101. For the last 100 steps: average loss 0.2839362920262279, accuracy 90\n",
            "Step 2201. For the last 100 steps: average loss 0.24859512037715426, accuracy 96\n",
            "Step 2301. For the last 100 steps: average loss 0.23663162413920585, accuracy 91\n",
            "Step 2401. For the last 100 steps: average loss 0.17352340431810473, accuracy 95\n",
            "Step 2501. For the last 100 steps: average loss 0.43984304988879297, accuracy 92\n",
            "Step 2601. For the last 100 steps: average loss 0.1568243978774192, accuracy 95\n",
            "Step 2701. For the last 100 steps: average loss 0.3404804459218959, accuracy 92\n",
            "Step 2801. For the last 100 steps: average loss 0.121584288139761, accuracy 99\n",
            "Step 2901. For the last 100 steps: average loss 0.15558330427906777, accuracy 94\n",
            "Step 3001. For the last 100 steps: average loss 0.3321828949342212, accuracy 87\n",
            "Step 3101. For the last 100 steps: average loss 0.2762771507756695, accuracy 91\n",
            "Step 3201. For the last 100 steps: average loss 0.20702283130984342, accuracy 95\n",
            "Step 3301. For the last 100 steps: average loss 0.31210234795708314, accuracy 91\n",
            "Step 3401. For the last 100 steps: average loss 0.22832730913010948, accuracy 95\n",
            "Step 3501. For the last 100 steps: average loss 0.13965901152327342, accuracy 96\n",
            "Step 3601. For the last 100 steps: average loss 0.24822396946504977, accuracy 91\n",
            "Step 3701. For the last 100 steps: average loss 0.36406806280707427, accuracy 87\n",
            "Step 3801. For the last 100 steps: average loss 0.41367803902855954, accuracy 85\n",
            "Step 3901. For the last 100 steps: average loss 0.2155166792083423, accuracy 96\n",
            "Step 4001. For the last 100 steps: average loss 0.3806326147393987, accuracy 87\n",
            "Step 4101. For the last 100 steps: average loss 0.13734242483449866, accuracy 95\n",
            "Step 4201. For the last 100 steps: average loss 0.30974095395585216, accuracy 90\n",
            "Step 4301. For the last 100 steps: average loss 0.15597060225141596, accuracy 97\n",
            "Step 4401. For the last 100 steps: average loss 0.4840561070549639, accuracy 87\n",
            "Step 4501. For the last 100 steps: average loss 0.4540295499455507, accuracy 87\n",
            "Step 4601. For the last 100 steps: average loss 0.13454955305579172, accuracy 96\n",
            "Step 4701. For the last 100 steps: average loss 0.16046536672580786, accuracy 96\n",
            "Step 4801. For the last 100 steps: average loss 0.2742579574064979, accuracy 95\n",
            "Step 4901. For the last 100 steps: average loss 0.3614353073861018, accuracy 85\n",
            "Epoch 3 ->\n",
            "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
            "Step 101. For the last 100 steps: average loss 0.14116910499313567, accuracy 97\n",
            "Step 201. For the last 100 steps: average loss 0.1901538339402938, accuracy 94\n",
            "Step 301. For the last 100 steps: average loss 0.14517926468613707, accuracy 96\n",
            "Step 401. For the last 100 steps: average loss 0.25419523980814496, accuracy 96\n",
            "Step 501. For the last 100 steps: average loss 0.4070260424628613, accuracy 89\n",
            "Step 601. For the last 100 steps: average loss 0.2741549436435564, accuracy 91\n",
            "Step 701. For the last 100 steps: average loss 0.19391422939874958, accuracy 94\n",
            "Step 801. For the last 100 steps: average loss 0.28011652063557874, accuracy 88\n",
            "Step 901. For the last 100 steps: average loss 0.15503632891266395, accuracy 94\n",
            "Step 1001. For the last 100 steps: average loss 0.2420021163496178, accuracy 93\n",
            "Step 1101. For the last 100 steps: average loss 0.1879189731656845, accuracy 96\n",
            "Step 1201. For the last 100 steps: average loss 0.18160063756033626, accuracy 95\n",
            "Step 1301. For the last 100 steps: average loss 0.23222291183399385, accuracy 91\n",
            "Step 1401. For the last 100 steps: average loss 0.2399205709755944, accuracy 95\n",
            "Step 1501. For the last 100 steps: average loss 0.2196154667491341, accuracy 93\n",
            "Step 1601. For the last 100 steps: average loss 0.13001168569209734, accuracy 96\n",
            "Step 1701. For the last 100 steps: average loss 0.1955799162832072, accuracy 95\n",
            "Step 1801. For the last 100 steps: average loss 0.09555684636775842, accuracy 98\n",
            "Step 1901. For the last 100 steps: average loss 0.16308635879890437, accuracy 95\n",
            "Step 2001. For the last 100 steps: average loss 0.15333184772876074, accuracy 95\n",
            "Step 2101. For the last 100 steps: average loss 0.47448602204812657, accuracy 92\n",
            "Step 2201. For the last 100 steps: average loss 0.13853601072888616, accuracy 95\n",
            "Step 2301. For the last 100 steps: average loss 0.27011319505689846, accuracy 94\n",
            "Step 2401. For the last 100 steps: average loss 0.3682287494223307, accuracy 89\n",
            "Step 2501. For the last 100 steps: average loss 0.16947366323219362, accuracy 94\n",
            "Step 2601. For the last 100 steps: average loss 0.2397234763412398, accuracy 91\n",
            "Step 2701. For the last 100 steps: average loss 0.17224880427218703, accuracy 94\n",
            "Step 2801. For the last 100 steps: average loss 0.21849084215652045, accuracy 95\n",
            "Step 2901. For the last 100 steps: average loss 0.23736537666317903, accuracy 92\n",
            "Step 3001. For the last 100 steps: average loss 0.2669919999547874, accuracy 90\n",
            "Step 3101. For the last 100 steps: average loss 0.21745558465506903, accuracy 92\n",
            "Step 3201. For the last 100 steps: average loss 0.3061563387053148, accuracy 90\n",
            "Step 3301. For the last 100 steps: average loss 0.1905636341978091, accuracy 94\n",
            "Step 3401. For the last 100 steps: average loss 0.19779665589974788, accuracy 95\n",
            "Step 3501. For the last 100 steps: average loss 0.29380168093438097, accuracy 90\n",
            "Step 3601. For the last 100 steps: average loss 0.434413268597513, accuracy 94\n",
            "Step 3701. For the last 100 steps: average loss 0.18124088986276285, accuracy 94\n",
            "Step 3801. For the last 100 steps: average loss 0.167265022401918, accuracy 95\n",
            "Step 3901. For the last 100 steps: average loss 0.24604720371826422, accuracy 92\n",
            "Step 4001. For the last 100 steps: average loss 0.2966940553942648, accuracy 90\n",
            "Step 4101. For the last 100 steps: average loss 0.12725461192429663, accuracy 97\n",
            "Step 4201. For the last 100 steps: average loss 0.2176453754428849, accuracy 94\n",
            "Step 4301. For the last 100 steps: average loss 0.19136176942097738, accuracy 93\n",
            "Step 4401. For the last 100 steps: average loss 0.2477868217163003, accuracy 93\n",
            "Step 4501. For the last 100 steps: average loss 0.20935074119502, accuracy 95\n",
            "Step 4601. For the last 100 steps: average loss 0.32004181856043123, accuracy 91\n",
            "Step 4701. For the last 100 steps: average loss 0.24017162389997815, accuracy 92\n",
            "Step 4801. For the last 100 steps: average loss 0.10286548475288475, accuracy 96\n",
            "Step 4901. For the last 100 steps: average loss 0.10302545376864264, accuracy 100\n",
            "Epoch 4 ->\n",
            "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
            "Step 101. For the last 100 steps: average loss 0.08304986012752304, accuracy 97\n",
            "Step 201. For the last 100 steps: average loss 0.14812887881722334, accuracy 97\n",
            "Step 301. For the last 100 steps: average loss 0.22473061314622533, accuracy 93\n",
            "Step 401. For the last 100 steps: average loss 0.2089630767704094, accuracy 94\n",
            "Step 501. For the last 100 steps: average loss 0.22020172518538106, accuracy 92\n",
            "Step 601. For the last 100 steps: average loss 0.19193338673582094, accuracy 92\n",
            "Step 701. For the last 100 steps: average loss 0.12738576647171107, accuracy 97\n",
            "Step 801. For the last 100 steps: average loss 0.1409946784617187, accuracy 94\n",
            "Step 901. For the last 100 steps: average loss 0.36411417907337346, accuracy 92\n",
            "Step 1001. For the last 100 steps: average loss 0.18913407648728014, accuracy 92\n",
            "Step 1101. For the last 100 steps: average loss 0.2652835386130869, accuracy 93\n",
            "Step 1201. For the last 100 steps: average loss 0.18691219468632628, accuracy 94\n",
            "Step 1301. For the last 100 steps: average loss 0.12790270943063853, accuracy 95\n",
            "Step 1401. For the last 100 steps: average loss 0.24051281146903425, accuracy 93\n",
            "Step 1501. For the last 100 steps: average loss 0.11270947656471543, accuracy 98\n",
            "Step 1601. For the last 100 steps: average loss 0.25319903177554154, accuracy 94\n",
            "Step 1701. For the last 100 steps: average loss 0.17906192646188654, accuracy 94\n",
            "Step 1801. For the last 100 steps: average loss 0.22081062115786637, accuracy 93\n",
            "Step 1901. For the last 100 steps: average loss 0.1283669857926039, accuracy 97\n",
            "Step 2001. For the last 100 steps: average loss 0.1869259564869201, accuracy 94\n",
            "Step 2101. For the last 100 steps: average loss 0.4489143398278674, accuracy 88\n",
            "Step 2201. For the last 100 steps: average loss 0.2935758650979971, accuracy 91\n",
            "Step 2301. For the last 100 steps: average loss 0.19758809137183533, accuracy 95\n",
            "Step 2401. For the last 100 steps: average loss 0.09335837575505389, accuracy 98\n",
            "Step 2501. For the last 100 steps: average loss 0.27335527575919655, accuracy 90\n",
            "Step 2601. For the last 100 steps: average loss 0.07437093011658895, accuracy 99\n",
            "Step 2701. For the last 100 steps: average loss 0.08873973813970475, accuracy 98\n",
            "Step 2801. For the last 100 steps: average loss 0.12283560712754477, accuracy 96\n",
            "Step 2901. For the last 100 steps: average loss 0.12299800664023118, accuracy 98\n",
            "Step 3001. For the last 100 steps: average loss 0.0891050573340054, accuracy 98\n",
            "Step 3101. For the last 100 steps: average loss 0.11194152513926746, accuracy 98\n",
            "Step 3201. For the last 100 steps: average loss 0.1798448746909704, accuracy 93\n",
            "Step 3301. For the last 100 steps: average loss 0.12473001578573703, accuracy 98\n",
            "Step 3401. For the last 100 steps: average loss 0.1434164444994589, accuracy 97\n",
            "Step 3501. For the last 100 steps: average loss 0.21517367327834738, accuracy 90\n",
            "Step 3601. For the last 100 steps: average loss 0.13745502423658362, accuracy 96\n",
            "Step 3701. For the last 100 steps: average loss 0.06847888547136566, accuracy 99\n",
            "Step 3801. For the last 100 steps: average loss 0.18767291185700066, accuracy 95\n",
            "Step 3901. For the last 100 steps: average loss 0.12223285841360262, accuracy 97\n",
            "Step 4001. For the last 100 steps: average loss 0.21018728669096085, accuracy 95\n",
            "Step 4101. For the last 100 steps: average loss 0.24590193680169248, accuracy 94\n",
            "Step 4201. For the last 100 steps: average loss 0.2259915856638435, accuracy 93\n",
            "Step 4301. For the last 100 steps: average loss 0.14843540806725591, accuracy 96\n",
            "Step 4401. For the last 100 steps: average loss 0.12823268037380614, accuracy 98\n",
            "Step 4501. For the last 100 steps: average loss 0.35820915121316566, accuracy 91\n",
            "Step 4601. For the last 100 steps: average loss 0.2202172247712666, accuracy 93\n",
            "Step 4701. For the last 100 steps: average loss 0.22130255793247242, accuracy 94\n",
            "Step 4801. For the last 100 steps: average loss 0.16328405160119933, accuracy 97\n",
            "Step 4901. For the last 100 steps: average loss 0.1586637997582385, accuracy 94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)\n",
        "\n",
        "\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "y = y.view(y.shape[0], 1)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "    y_predicted = model(X)\n",
        "    loss = criterion(y_predicted, y)\n",
        "    \n",
        "   \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "\n",
        "predicted = model(X).detach().numpy()\n",
        "\n",
        "plt.plot(X_numpy, y_numpy, 'ro')\n",
        "plt.plot(X_numpy, predicted, 'b')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "a9eC1ObTDVp2",
        "outputId": "a848fed3-1311-4be5-b69b-dad004dd1987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss = 4047.6594\n",
            "epoch: 20, loss = 2852.7334\n",
            "epoch: 30, loss = 2038.1647\n",
            "epoch: 40, loss = 1482.7622\n",
            "epoch: 50, loss = 1103.9894\n",
            "epoch: 60, loss = 845.6217\n",
            "epoch: 70, loss = 669.3495\n",
            "epoch: 80, loss = 549.0634\n",
            "epoch: 90, loss = 466.9656\n",
            "epoch: 100, loss = 410.9220\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}